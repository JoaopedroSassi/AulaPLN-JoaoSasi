# -*- coding: utf-8 -*-
"""Aula 03 - 28/02 -- Joao Sassi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JcZCt3ZceyjejIGMFE--E5nRh8-fOqL6

# Aula 3

## 1. Normaliza√ß√£o de texto e Remo√ß√£o de Ru√≠do
"""

import re

original = "Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas."

texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\s]', '',original)

texto_normalizado = texto_limpo.lower()

print(f'Texto original: {original}')
print(f'\nTexto limpo: {texto_limpo}')
print(f'\nTexto normalizado: {texto_normalizado}')

"""## 2. Tokeniza√ß√£o

"""

import nltk
from nltk.tokenize import word_tokenize

#nltk.download('punkt_tab')

word_tokenize(texto_normalizado)

print(f'Texto original: {original}')
print(f'\n\nTexto limpo: {texto_limpo}')
print(f'\n\nTexto normalizado: {texto_normalizado}')
print(f'\n\nTokens extraidos: {tokens}\n')

"""## 3. Remo√ß√£o de Stopwords"""

from nltk.corpus import stopwords

#nltk.download('stopwords')

stopwords_pt = set(stopwords.words('portuguese'))

print(stopwords_pt)

tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]

print(f'\n\nTokens extraidos: {tokens} + \n quantidade de tokens: {len(tokens)}')
print(f'\n\nTokens extraidos: {tokens_sem_stopwords} + \n quantidade de tokens: {len(tokens_sem_stopwords)}\n')

"""## 4. Stemming e Lemaliza√ß√£o"""

from nltk.stem import RSLPStemmer

# nltk.download('rslp')

stemmer = RSLPStemmer()
stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]
print(f'\n\nTokens extraidos: {tokens_sem_stopwords}')
print(f'\n\nTokens radicais: {stemming}\n\n\n')

"""## 5. Exemplo 01 - Pr√© Processamento completo"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

#nltk.download('punkt')
#nltk.download('stopwords')

texto = input("Insira um texto que seja coerente, podendo ter s√≠mbolos: ")

texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)
texto_lower = texto_limpo.lower()

tokens = nltk.word_tokenize(texto_lower)

stop_words = set(stopwords.words('portuguese'))
palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]

stemmer = PorterStemmer()
palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]

print(palavras_stemizadas)

"""## Exemplo 02 - Estrutura de Pr√©-processamento de texto"""

!pip install spacy
!python -m spacy download pt_core_news_sm

import re
import spacy
import nltk
from nltk.corpus import stopwords
import string

#nltk.download('stopwords')
#nltk.download('punkt_tab')
#nltk.download('wordnet')
#nltk.download('rslp')

# (Trocar para 'en_core_web_sm' se for ingl√™s)
nlp = spacy.load("pt_core_news_sm")

texto = "O Processamento de Linguagem Natural (PLN) √© uma √°rea essencial da intelig√™ncia artificial! üòä Confira mais em: https://exemplo.com"

def normalizar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'https?://\S+|www\.\S+', '', texto)
    texto = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö√ß√á ]', '', texto)
    return texto

texto_normalizado = normalizar_texto(texto)

tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')

stopwords_pt = set(stopwords.words('portuguese'))
tokens_sem_stopwords = [token for token in tokens if token not in stopwords_pt]

stemmer = nltk.RSLPStemmer()
tokens_stem = [stemmer.stem(token) for token in tokens_sem_stopwords]

def lematizar_com_spacy(tokens):
    doc = nlp(" ".join(tokens))
    return [token.lemma_ for token in doc]

tokens_lematizados = lematizar_com_spacy(tokens_sem_stopwords)

print("Texto Original:\n", texto)
print("\nTexto Normalizado:\n", texto_normalizado)
print("\nTokens:\n", tokens)
print("\nTokens Sem Stopwords:\n", tokens_sem_stopwords)
print("\nStemming:\n", tokens_stem)
print("\nLematiza√ß√£o:\n", tokens_lematizados)

"""## Exemplo 03 - O modelo pre treinado"""

import spacy

nlp = spacy.load("pt_core_news_sm")

textoRecebido = input("Digite um texto para ser analisado: ")
doc = nlp(textoRecebido)

print('\nAn√°lise gramatical das palavras:')
for token in doc:
    print(f"Palavra: {token.text}, Classe: {token.pos_}")

print("\nAnalise de Depend√™ncias:")
for token in doc:
  print(f"Palavra: {token.text}, Depende de: {token.head.text}")

from spacy import displacy
displacy.render(doc, style="dep", jupyter=True)